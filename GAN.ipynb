{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Input,Dense,Flatten,Reshape\n",
    "from keras.models import Sequential,Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"330_Days_Data.csv\"\n",
    "dataset = pd.read_csv(filename,sep=\",\")\n",
    "data = np.array(dataset)\n",
    "data = data[:,1:]\n",
    "\n",
    "n_features = np.shape(data)[1]\n",
    "n_days = int(filename.split(\"_\")[0])\n",
    "n_days_used = 30\n",
    "n_samples = int(len(data)/n_days)\n",
    "percentage = 0.7\n",
    "train_data = data[:int(n_samples*percentage)*n_days]\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(train_data)\n",
    "\n",
    "train_data = scaler.transform(train_data)\n",
    "data_dim = [n_days_used,n_features]\n",
    "train_data = np.reshape(train_data,(-1,data_dim[0]*(data_dim[1])))\n",
    "minibatch = np.zeros((11,7,205,270))\n",
    "for sample in range(len(train_data)):\n",
    "    minibatch[sample%11][int(sample/(11*205))][int((sample/11)%205)] = train_data[sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        # Data and Noise dimensions\n",
    "        self.data_rows = 1\n",
    "        self.data_coll = 9\n",
    "        self.data_shape = (self.data_rows,self.data_coll)\n",
    "        self.latent_dim = 20\n",
    "        \n",
    "        # Using Adam Optimizer with lr = 0.0005\n",
    "        optimizer = Adam(0.0005)\n",
    "        \n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss ='binary_crossentropy',optimizer=optimizer,metrics=['accuracy']) \n",
    "        self.generator = self.build_generator()\n",
    "        \n",
    "        # The generator transforms noise in data \n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        data = self.generator(z)\n",
    "        \n",
    "        # Train only the generator on the combined model\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        valid_or_not = self.discriminator(data)\n",
    "        \n",
    "        self.comb = Model(z, valid_or_not)\n",
    "        self.comb.compile(loss='binary_crossentropy',optimizer=optimizer)\n",
    "    \n",
    "    def build_generator(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, activation = \"relu\", input_dim = self.latent_dim))\n",
    "        model.add(Dense(512, activation = \"relu\"))\n",
    "        model.add(Dense(9, activation = \"sigmoid\"))\n",
    "        model.add(Reshape(self.data_shape))\n",
    "        model.summary()\n",
    "        \n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        data = model(noise)\n",
    "        \n",
    "        return Model(noise, data)\n",
    "    \n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape = self.data_shape))\n",
    "        model.add(Dense(512, activation = \"relu\"))\n",
    "        model.add(Dense(256, activation = \"relu\"))\n",
    "        model.add(Dense(1, activation = \"sigmoid\"))\n",
    "        model.summary()\n",
    "        \n",
    "        data = Input(shape = self.data_shape)\n",
    "        result = model(data)\n",
    "        \n",
    "        return Model(data, result)\n",
    "    \n",
    "    def save_data(self, global_step, file_name):\n",
    "        gen_noise = np.random.normal(0,1,(387450, self.latent_dim))\n",
    "        gen_data = self.generator.predict(gen_noise)\n",
    "        generated_sample = np.reshape(gen_data,(387450,9))\n",
    "        np.savetxt(\"generated_samples_\"+file_name+\"_\"+str(global_step)+\".csv\",generated_sample,delimiter = \",\")\n",
    "    \n",
    "    def train(self, global_steps, file_name, batch_size = 205, checkpoint=1000):\n",
    "            # The labels for original data are 1 and the labels for fake data are 0\n",
    "            original = np.ones((batch_size, 1))\n",
    "            fake = np.zeros((batch_size, 1))\n",
    "            \n",
    "            for global_step in range(global_steps):\n",
    "                \n",
    "                gen_noise = np.random.normal(0, 1, (205*30,self.latent_dim))\n",
    "                idx = np.random.randint(0,7)\n",
    "                # Use only data from the last month class\n",
    "                o_data = minibatch[10][idx]\n",
    "                noise = gen_noise\n",
    "                o_data = np.reshape(o_data, newshape = [-1,1,9])\n",
    "                \n",
    "                gen_data = self.generator.predict(noise)\n",
    "                \n",
    "                # Train the discriminator network\n",
    "                d_loss_real = self.discriminator.train_on_batch(o_data, original)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(gen_data, fake)\n",
    "                d_total_loss = np.add(d_loss_real, d_loss_fake)/2\n",
    "                \n",
    "                # Training the Generator\n",
    "                # The generator wants it's labels to be 1 so it can fool the discriminator\n",
    "                g_loss = self.comb.train_on_batch(noise, original) \n",
    "                \n",
    "                print(\"Global Step :%d | Discriminator loss : %f | Generator loss : %f | Discriminator Acc : %f\"%(global_step, d_total_loss[0], g_loss, 100*d_total_loss[1]))\n",
    "                \n",
    "                # Save sintetic samples\n",
    "                if (global_step % checkpoint == 0):\n",
    "                    self.save_data(global_step,file_name)\n",
    "                \n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    gan = GAN()\n",
    "    gan.train(global_steps = 50001, file_name = str(i), batch_size = 205*30, checkpoint = 10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
