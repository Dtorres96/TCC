{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf(train_data,train_labels,test_data,test_labels):\n",
    "    test_size = int(len(labels_test)/2)\n",
    "    best_test_acc = 0\n",
    "    best_val_acc = 0\n",
    "    macro_p = 0\n",
    "    micro_p = 0\n",
    "    macro_f1 = 0 \n",
    "    micro_f1 = 0\n",
    "    macro_r = 0\n",
    "    micro_r = 0\n",
    "    r = 0\n",
    "    p = 0\n",
    "    f1 = 0\n",
    "    for depth in range(1,35):\n",
    "        clf = RandomForestClassifier(n_estimators=100, max_depth=depth,random_state=0)\n",
    "        clf.fit(train_data,train_labels)\n",
    "        print('Max Depth',depth)\n",
    "        print('Train Acc',clf.score(train_data,train_labels))\n",
    "        print('Test Acc',clf.score(test_data_balanced[:test_size],test_labels[:test_size]))\n",
    "        print('Validation Acc',clf.score(test_data_balanced[test_size:],test_labels[test_size:]))\n",
    "\n",
    "        if(clf.score(test_data_balanced[test_size:],test_labels[test_size:]) >= best_val_acc):\n",
    "            best_val_acc = clf.score(test_data_balanced[test_size:],test_labels[test_size:])\n",
    "            best_test_acc = clf.score(test_data_balanced[:test_size],test_labels[:test_size])\n",
    "\n",
    "            macro_p = precision_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]) , average='macro')\n",
    "            micro_p = precision_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]) , average='micro')\n",
    "\n",
    "            macro_f1 = f1_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]) , average='macro')\n",
    "            micro_f1 = f1_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]) , average='micro')\n",
    "\n",
    "            macro_r = recall_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]) , average='macro')\n",
    "            micro_r = recall_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]) , average='micro')\n",
    "\n",
    "            p = precision_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]))\n",
    "            r = recall_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]))\n",
    "            f1 = f1_score(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]))\n",
    "\n",
    "            cf = confusion_matrix(test_labels[:test_size],clf.predict(test_data_balanced[:test_size]))\n",
    "    print(\"macro_p \",macro_p)\n",
    "    print(\"micro_p \",micro_p)\n",
    "    print(\"macro_r \",macro_r)\n",
    "    print(\"micro_r \",micro_r)\n",
    "    print(\"macro_f1 \",macro_f1)\n",
    "    print(\"micro_f1 \",micro_f1)\n",
    "    print(\"precission \",p)\n",
    "    print(\"recall \",r)\n",
    "    print(\"f1 \",f1)\n",
    "    print(cf)\n",
    "    return best_val_acc,best_test_acc\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running 5 Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inter in range(5):\n",
    "    filename = \"330_Days_Data.csv\"\n",
    "    dataset = pd.read_csv(filename,sep=\",\")\n",
    "    data = np.array(dataset)\n",
    "    data = data[:,1:]\n",
    "\n",
    "    gen_filename = 'generated_samples_'+str(inter)+'_50000.csv'\n",
    "    gen_samples = np.genfromtxt(gen_filename,delimiter=\",\")\n",
    "    gen_samples = gen_samples.reshape(-1,30,9)\n",
    "\n",
    "\n",
    "    n_features = np.shape(data)[1]\n",
    "    n_days = int(filename.split(\"_\")[0])\n",
    "    n_days_used = 30\n",
    "    n_samples = int(len(data)/n_days)\n",
    "    percentage = 0.7\n",
    "    train_data = data[:int(n_samples*percentage)*n_days]\n",
    "    test_data = data[int(n_samples*percentage)*n_days:]\n",
    "\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(train_data)\n",
    "    train_data = scaler.transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "   \n",
    "\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "    train_data_balanced = []\n",
    "    test_data_balanced = []\n",
    "    extra_data = []\n",
    "    counter = 0\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        if(i%330 >= 300):\n",
    "            train_data_balanced.append(train_data[i])\n",
    "            labels_train.append(1)\n",
    "        else:\n",
    "            train_data_balanced.append(train_data[i])\n",
    "            labels_train.append(0)\n",
    "            \n",
    "    for i in range(len(test_data)):\n",
    "        if(i%330 >= 300):\n",
    "            test_data_balanced.append(test_data[i])\n",
    "            labels_test.append(1)\n",
    "        else:\n",
    "            test_data_balanced.append(test_data[i])\n",
    "            labels_test.append(0)\n",
    "    \n",
    "    gen_samples = np.reshape(gen_samples,(-1,9))\n",
    "    label_gen = np.ones(len(gen_samples))\n",
    "\n",
    "    aug_data = np.vstack((gen_samples,train_data_balanced))\n",
    "    aug_label = np.hstack((label_gen,labels_train))\n",
    "\n",
    "    sm = SMOTE(random_state=inter)\n",
    "    smote_samples, smote_labels = sm.fit_resample(train_data_balanced, labels_train)\n",
    "\n",
    "    ad = ADASYN(random_state=inter)\n",
    "    adasyn_samples, adasyn_labels = ad.fit_resample(train_data_balanced, labels_train)\n",
    "\n",
    "    rd = RandomOverSampler(random_state=inter)\n",
    "    rd_samples, rd_labels = rd.fit_resample(train_data_balanced, labels_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"data augmented by gan\")\n",
    "    gan =train_rf(aug_data,aug_label,test_data_balanced,labels_test)\n",
    "    print(\"data augmented by SMOTE\")\n",
    "    smote = train_rf(smote_samples,smote_labels,test_data_balanced,labels_test)\n",
    "    print(\"data augmented by ADASYN\")\n",
    "    adasyn = train_rf(adasyn_samples,adasyn_labels,test_data_balanced,labels_test)\n",
    "    print(\"data augmented by Random Oversampler\")\n",
    "    random_oversampling = train_rf(rd_samples,rd_labels,test_data_balanced,labels_test)\n",
    "    print(\"original data\")\n",
    "    original = train_rf(train_data_balanced,labels_train,test_data_balanced,labels_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
